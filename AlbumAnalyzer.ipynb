{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Album Analyzer\n",
    "\n",
    "\n",
    "This notebook will take the artist name and album name of a given artist and album and will query the user to confirm the album they want to select. \n",
    "\n",
    "\n",
    "The notebook will then grab all the lyrics from the album and perform sentiment analysis on it, then it will graph the sentiment analysis and create a word cloud. I am planning on adding a few more features, but for what it is right now, this is what it'll do.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install lyricsgenius\n",
    "# !pip install vaderSentiment\n",
    "# !pip install nltk\n",
    "import nltk\n",
    "import json\n",
    "import requests\n",
    "nltk.download('vader_lexicon')\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from wordcloud import WordCloud\n",
    "import matplotlib.pyplot as plt\n",
    "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from lyricsgenius import Genius\n",
    "from PIL import Image\n",
    "from wordcloud import WordCloud, STOPWORDS, ImageColorGenerator\n",
    "import numpy as np \n",
    "import requests\n",
    "from io import BytesIO \n",
    "import matplotlib.pyplot as plt\n",
    "from wordcloud import WordCloud\n",
    "import os\n",
    "from dotenv import load_dotenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load api key from .env file\n",
    "load_dotenv()\n",
    "genius = Genius('GENIUS_API_KEY')\n",
    "\n",
    "#ask the user to input an artist and an album\n",
    "artist = input(\"Enter an artist: \")\n",
    "album = input(\"Enter an album: \")\n",
    "\n",
    "#set up genius api\n",
    "# genius = Genius('-Xfliv9Xg0cLJLkKUWL5ebJTkbhUQC2lI3TuW4Ai6dTu-f75ONHDy37mTUoAzzVh')\n",
    "\n",
    "#parameters for the api call\n",
    "genius.remove_section_headers = True\n",
    "genius.excluded_terms = [\"(Remix)\", \"(Live)\"]\n",
    "genius.skip_non_songs = True\n",
    "\n",
    "#search for the album\n",
    "album = genius.search_albums(str(artist) + \" \" + str(album))\n",
    "\n",
    "album_names = []\n",
    "album_ids = []\n",
    "for i in range(len(album['sections'][0]['hits'])):\n",
    "    album_names.append(album['sections'][0]['hits'][i]['result']['name'])\n",
    "    album_ids.append(album['sections'][0]['hits'][i]['result']['id'])\n",
    "album_df = pd.DataFrame({'album_name': album_names, 'album_id': album_ids})\n",
    "album_df['choice'] = album_df.index\n",
    "album_df['choice'] = album_df['choice'] + 1\n",
    "\n",
    "album_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_album(choice):\n",
    "    album_name = album_df.loc[album_df['choice'] == choice, 'album_name'].iloc[0]\n",
    "    album_id = album_df.loc[album_df['choice'] == choice, 'album_id'].iloc[0]\n",
    "    return album_name, album_id\n",
    "\n",
    "#ask the user to input a choice\n",
    "choice = int(input(\"Enter a choice: \"))\n",
    "#call the get_album function\n",
    "album_name, album_id = get_album(choice)\n",
    "#display the album name and id``\n",
    "print(album_name)\n",
    "print(album_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "album = genius.search_album(str(artist) + \" \" + str(album), album_id=album_id)\n",
    "album.save_lyrics('lyrics.json', overwrite=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('lyrics.json') as f:\n",
    "    data = json.load(f)\n",
    "    print(json.dumps(data, indent=4, sort_keys=True))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#display the cover_art_thumbnail_url as in image from the web in a jupyter notebook and download it\n",
    "from IPython.display import Image\n",
    "from IPython.core.display import HTML\n",
    "cover_art = Image(url= data['cover_art_url'])\n",
    "cover_art\n",
    "\n",
    "\n",
    "## Download the album image locally\n",
    "\n",
    "headers = {'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_10_1) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/39.0.2171.95 Safari/537.36'}\n",
    "url = data['cover_art_url']\n",
    "response = requests.get(url, headers=headers)\n",
    "\n",
    "#save response as a file\n",
    "with open('cover_art.png', 'wb') as f:\n",
    "    f.write(response.content)   \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#read in the lyrics\n",
    "with open('lyrics.json') as f:\n",
    "    data = json.load(f)\n",
    "    print(json.dumps(data, indent=4, sort_keys=True))\n",
    "    print(data.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#read the json file and print in a readable format and print keys\n",
    "import json\n",
    "with open('lyrics.json') as f:\n",
    "    data = json.load(f)\n",
    "    print(json.dumps(data, indent=4, sort_keys=True))\n",
    "    print(data.keys())\n",
    "\n",
    "#count the number of ids in the json file\n",
    "print(len(data['tracks']))\n",
    "\n",
    "#print the keys under 'tracks' in the json file\n",
    "print(data['tracks'][0].keys())\n",
    "\n",
    "#print the keys under 'song' in the json file\n",
    "print(data['tracks'][0]['song'].keys())\n",
    "\n",
    "#print the lyrics under each song in the json file\n",
    "for i in range(len(data['tracks'])):\n",
    "    print(data['tracks'][i]['song']['lyrics'])\n",
    "    \n",
    "\n",
    "#add each song and its lyrics to a dataframe\n",
    "df = pd.DataFrame(columns=['song', 'lyrics'])\n",
    "for i in range(len(data['tracks'])):\n",
    "    df = df.append({'song': data['tracks'][i]['song']['title'], 'lyrics': data['tracks'][i]['song']['lyrics']}, ignore_index=True)\n",
    "df\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#each lyric is a string that starts with the song title, remove the song title\n",
    "df['lyrics'] = df['lyrics'].str.replace(r'^.*\\n', '')\n",
    "df\n",
    "\n",
    "#remove all punctuation, convert to lowercase, and remove stopwords, include the word 'chorus'\n",
    "stop_words = set(stopwords.words('english'))\n",
    "newStopWords = ['chorus','embed', 'like2embed']\n",
    "stop_words.update(newStopWords)\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "tokenizer = RegexpTokenizer(r'\\w+')\n",
    "df['lyrics'] = df['lyrics'].apply(lambda x: ' '.join([lemmatizer.lemmatize(word.lower()) for word in tokenizer.tokenize(x) if word not in stop_words]))\n",
    "df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#for each song, calculate the sentiment score and add it to a new column\n",
    "analyzer = SentimentIntensityAnalyzer()\n",
    "df['sentiment_score'] = df['lyrics'].apply(lambda x: analyzer.polarity_scores(x)['compound'])\n",
    "df\n",
    "\n",
    "#perform sentiment analysis on the lyrics of each song, and add the result to a new column, sentiment\n",
    "df['sentiment'] = df['sentiment_score'].apply(lambda x: 'positive' if x > 0 else 'negative' if x < 0 else 'neutral')\n",
    "df\n",
    "\n",
    "#tokenize the lyrics of each song, split the lyrics into a list of words and add it to a new column\n",
    "df['tokenized_lyrics'] = df['lyrics'].apply(lambda x: word_tokenize(x))\n",
    "df\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#graph the sentiment analysis of each song\n",
    "sns.set_style('darkgrid')\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.title('Sentiment Analysis of ' + str(album_name))\n",
    "sns.barplot(x='song', y='sentiment_score', data=df)\n",
    "plt.xticks(rotation=90)\n",
    "plt.show()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#graph the sentiment analysis of the album including the average sentiment score\n",
    "sns.set_style('darkgrid')\n",
    "plt.figure(figsize=(10, 5))\n",
    "sns.countplot(x='sentiment', data=df)\n",
    "plt.title('Sentiment Analysis of ' + str(album_name))\n",
    "plt.xlabel('Sentiment')\n",
    "plt.ylabel('Count')\n",
    "plt.show()\n",
    "print('Average Sentiment Score: ' + str(df['sentiment_score'].mean()))\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#make a word cloud of the lyrics of the album\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.title('Word Cloud of ' + str(album_name))\n",
    "wordcloud = WordCloud(width=800, height=400, background_color='white', max_words=100).generate(' '.join(df['lyrics']))\n",
    "plt.imshow(wordcloud)\n",
    "plt.axis('off')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#generate a wordcloud of the entire album\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.title('Word Cloud of ' + str(album_name))\n",
    "wordcloud = WordCloud(width=800, height=400, background_color='white', max_words=100).generate(' '.join(df['lyrics']))\n",
    "plt.imshow(wordcloud)\n",
    "plt.axis('off')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#combine all the lyrics into one string\n",
    "#import Image libraries\n",
    "#impot image libraries\n",
    "from PIL import Image\n",
    "\n",
    "words = ' '.join(df['lyrics'])\n",
    "\n",
    "def read_img_from_url(url, headers=headers):\n",
    "    response = requests.get(url, headers=headers)\n",
    "    img = Image.open(BytesIO(response.content))\n",
    "    img_matrix = np.array(img)\n",
    "    return img_matrix\n",
    "\n",
    "def read_txt(lyrics, *size):\n",
    "    text = words\n",
    "    #wc = WordCloud(background_color=\"white\", max_words=100 , max_font_size=100, width=size[0], height=size[1], random_state=42)\n",
    "    wc = WordCloud(max_words=2000, contour_width=3, width=size[0], height=size[1], contour_color='steelblue').generate(' '.join(df['lyrics']))\n",
    "    return wc.to_array()\n",
    "\n",
    "\n",
    "headers = {'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_10_1) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/39.0.2171.95 Safari/537.36'}\n",
    "img_url = data['cover_art_url']\n",
    "img_matrix = read_img_from_url(img_url)\n",
    "txt_url = words\n",
    "txt_matrix = read_txt(txt_url, *img_matrix.shape)\n",
    "\n",
    "print(img_matrix.shape, txt_matrix.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_matrix[txt_matrix == 100] = 0\n",
    "print(img_matrix.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#graph the wordcloud ontop of the image of the album cover\n",
    "plt.figure(figsize=(10, 10), dpi=300)\n",
    "plt.imshow(img_matrix)\n",
    "plt.imshow(txt_matrix, alpha=0.55)\n",
    "plt.axis('off')\n",
    "plt.show()\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#find the most infrequent words in the album\n",
    "from collections import Counter\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "import string\n",
    "\n",
    "stop_words = set(stopwords.words('english'))\n",
    "stop_words.add('chorus')\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "tokenizer = RegexpTokenizer(r'\\w+')\n",
    "words = ' '.join(df['lyrics'])\n",
    "words = [lemmatizer.lemmatize(word.lower()) for word in tokenizer.tokenize(words) if word not in stop_words]\n",
    "word_counts = Counter(words)\n",
    "word_counts.most_common(10)\n",
    "word_counts.most_common()[:-11:-1]\n",
    "\n",
    "#graph the most infrequent words in the album\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.title('Most Infrequent Words in ' + str(album_name))\n",
    "sns.barplot(x=[word[0] for word in word_counts.most_common()[:-11:-1]], y=[word[1] for word in word_counts.most_common()[:-11:-1]])\n",
    "plt.xticks(rotation=90)\n",
    "plt.show()\n",
    "\n",
    "#list the most infrequent words in the album\n",
    "print('Most Infrequent Words in ' + str(album_name))\n",
    "for word in word_counts.most_common()[:-21:-1]:\n",
    "    print(word[0] + ': ' + str(word[1]))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#find the mean sentiment analysis of the album\n",
    "df['sentiment_score'].mean()\n",
    "\n",
    "#add the mean sentiment of the album to a dataframe, include Artist name, album name, and the score\n",
    "album_sentiment = pd.DataFrame({'artist': [artist], 'album': [album_name], 'sentiment_score': [df['sentiment_score'].mean()]})\n",
    "album_sentiment\n",
    "\n",
    "#export the dataframe to a csv file, and append the new data to the existing csv file\n",
    "album_sentiment.to_csv('album_sentiment.csv', mode='a', header=False, index=False)\n",
    "\n",
    "#read the csv file\n",
    "album_sentiment = pd.read_csv('album_sentiment.csv')\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.8 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "5d1e37ab42fdcd25737c9d7ec581bdd15084cf0e3ae43de527cc420fc4ac3388"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
